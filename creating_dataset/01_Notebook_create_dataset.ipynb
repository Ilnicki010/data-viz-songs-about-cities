{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing libs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "from geopy import geocoders\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import glob, os, time\n",
    "import sys  \n",
    "cwd = os.getcwd()\n",
    "from Script_utils import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get all songs from U.S. and list separate articles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base_URL = 'https://en.wikipedia.org'\n",
    "all_songs_URL = base_URL + '/wiki/List_of_songs_about_cities'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = load_page(all_songs_URL)\n",
    "\n",
    "us_h2 = page_soup.select('h2 > span.mw-headline#United_States')[0].parent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cities = []\n",
    "states = []\n",
    "songs = []\n",
    "\n",
    "\n",
    "for element in us_h2.next_siblings:\n",
    "\n",
    "    if element.name == 'h2': break;\n",
    "    if element.name == 'h3':\n",
    "        city_state = get_city_and_state(element.getText())\n",
    "        start_point = element\n",
    "\n",
    "        for list_element in start_point.next_siblings:\n",
    "\n",
    "            if list_element.name == 'h3': break\n",
    "            if list_element.name == 'ul':\n",
    "\n",
    "                for song_element in list_element.select('li'):\n",
    "                    song_name = get_song_name(song_element.getText())\n",
    "                    songs.append(song_name)\n",
    "                    cities.append(city_state[0])\n",
    "                    states.append(city_state[1])\n",
    "\n",
    "            if list_element.name == 'div':\n",
    "                single_aricle_URL = list_element.select('a',herf=True)[0]['href']\n",
    "                print(single_aricle_URL)\n",
    "                continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- /wiki/List_of_songs_about_Atlanta âœ…\n",
    "- /wiki/List_of_songs_about_Birmingham,_Alabama ðŸš§\n",
    "- /wiki/List_of_songs_about_Boston âœ…\n",
    "- /wiki/List_of_songs_about_Chicago âœ…\n",
    "- /wiki/List_of_songs_about_Detroit âœ…\n",
    "- /wiki/List_of_songs_about_Los_Angeles ðŸš§\n",
    "- /wiki/List_of_songs_about_Miami âœ…\n",
    "- /wiki/List_of_songs_about_Nashville âœ…\n",
    "- /wiki/List_of_songs_about_New_Orleans âœ…\n",
    "- /wiki/List_of_songs_about_New_York_City âœ…\n",
    "- /wiki/List_of_songs_about_Portland,_Oregon âœ…\n",
    "- /wiki/List_of_songs_about_Seattle ðŸš§\n",
    "\n",
    "âœ… - csv is generated <br/>\n",
    "ðŸš§ - work in progress"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'Songs {len(songs)} | Cities: {len(cities)} | States: {len(states)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data = pd.DataFrame(data={'City':cities,'State':states,'Song':songs})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rows_with_nan = export_data[export_data.isnull().any(axis=1)]\n",
    "print(rows_with_nan)\n",
    "print(f'# of rows with NaN: {len(rows_with_nan)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There were 18 rows with NaN values coused by issues below:\n",
    "\n",
    "1. Missing cloasing quote\n",
    "- Details: I noticed some of the songs in the Wikipedia article don't have closing quote (!) and my regex fail.\n",
    "- Solution: I could make more complex regex to handle this situation, but instead I edit Wikipedia article. There are no so many of that rows and it'll help future scrapers :)\n",
    "\n",
    "2. Different quote type\n",
    "- Details: I found out there are many different type of quotation marks in UNICODE.\n",
    "- Solution: I need to sanitize song names a bit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data.to_csv(cwd + '/datasets/Data_main.csv',index=False)\n",
    "export_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scraping data for each big city\n",
    "\n",
    "Unfortunately, every article has a slightly different structure so I had to scrape them separately."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def scrape_single_aricle(URL, start_tag,start_id, stop_tag, city_name, state_name):\n",
    "    page_soup = load_page(base_URL + URL)\n",
    "\n",
    "    start = page_soup.find(start_tag,id=start_id).parent\n",
    "\n",
    "    songs = []\n",
    "    cities = []\n",
    "    states = []\n",
    "\n",
    "    for element in start.next_siblings:\n",
    "        if element.name == stop_tag: break\n",
    "        if element.name == 'ul':\n",
    "            for li in element.select('li'):\n",
    "                song_name = get_song_name(li.getText())\n",
    "                songs.append(song_name)\n",
    "                cities.append(city_name)\n",
    "                states.append(state_name)\n",
    "    return {\n",
    "        'songs':songs,\n",
    "        'cities':cities,\n",
    "        'states':states\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detroit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "detroit_results = scrape_single_aricle(\n",
    "    URL = '/wiki/List_of_songs_about_Detroit',\n",
    "    start_tag = 'span',\n",
    "    start_id='0-9',\n",
    "    stop_tag = 'div',\n",
    "    city_name = 'Detroit',\n",
    "    state_name = 'Michigan')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_detroit = pd.DataFrame(data={'City':detroit_results['cities'],'State':detroit_results['states'],'Song':detroit_results['songs']})\n",
    "export_data_detroit.to_csv(cwd + '/datasets/Data_detroit.csv',index=False)\n",
    "export_data_detroit.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Miami"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "miami_results = scrape_single_aricle(\n",
    "    URL = '/wiki/List_of_songs_about_Miami',\n",
    "    start_tag = 'span',\n",
    "    start_id='Songs_about_Miami',\n",
    "    stop_tag = 'h2',\n",
    "    city_name = 'Miami',\n",
    "    state_name = 'Florida')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_miami = pd.DataFrame(data={'City':miami_results['cities'],'State':miami_results['states'],'Song':miami_results['songs']})\n",
    "export_data_miami.to_csv(cwd + '/datasets/Data_miami.csv',index=False)\n",
    "export_data_miami.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New Orleans"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_orleans_results = scrape_single_aricle(\n",
    "    URL = '/wiki/List_of_songs_about_New_Orleans',\n",
    "    start_tag = 'span',\n",
    "    start_id='0-9',\n",
    "    stop_tag = 'div',\n",
    "    city_name = 'New Orleans',\n",
    "    state_name = 'Louisiana')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_new_orleans = pd.DataFrame(data={'City':new_orleans_results['cities'],'State':new_orleans_results['states'],'Song':new_orleans_results['songs']})\n",
    "export_data_new_orleans.to_csv(cwd + '/datasets/Data_new_orleans.csv',index=False)\n",
    "export_data_new_orleans.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Los Angeles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "los_angeles_results = scrape_single_aricle(\n",
    "    URL = '/wiki/List_of_songs_about_Los_Angeles',\n",
    "    start_tag = 'span',\n",
    "    start_id='#sâ€“A',\n",
    "    stop_tag = 'h2',\n",
    "    city_name = 'Los Angeles',\n",
    "    state_name = 'California')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_los_angeles = pd.DataFrame(data={'City':los_angeles_results['cities'],'State':los_angeles_results['states'],'Song':los_angeles_results['songs']})\n",
    "export_data_los_angeles.to_csv(cwd + '/datasets/Data_los_angeles.csv',index=False)\n",
    "export_data_los_angeles.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chicago"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "chicago_results = scrape_single_aricle(\n",
    "    URL = '/wiki/List_of_songs_about_Chicago',\n",
    "    start_tag = 'span',\n",
    "    start_id='0â€“9',\n",
    "    stop_tag = 'p',\n",
    "    city_name = 'Chicago',\n",
    "    state_name = 'Illinois')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_chicago = pd.DataFrame(data={'City':chicago_results['cities'],'State':chicago_results['states'],'Song':chicago_results['songs']})\n",
    "export_data_chicago.to_csv(cwd + '/datasets/Data_chicago.csv',index=False)\n",
    "export_data_chicago.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New York City"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = load_page(base_URL + '/wiki/List_of_songs_about_New_York_City')\n",
    "\n",
    "start = page_soup.find(\"span\",id=\"0â€“9\").parent\n",
    "\n",
    "songs = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "for element in start.next_siblings:\n",
    "    if element.name == 'p': break\n",
    "    if element.name == 'div':\n",
    "        el = element.select('ul')[0]\n",
    "        for li in el.select('li'):\n",
    "            song_name = get_song_name(li.getText())\n",
    "            songs.append(song_name)\n",
    "            cities.append('New York City')\n",
    "            states.append('New York')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_new_york_city = pd.DataFrame(data={'City':cities,'State':states,'Song':songs})\n",
    "export_data_new_york_city.to_csv(cwd + '/datasets/Data_new_york_city.csv',index=False)\n",
    "export_data_new_york_city.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nashville"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = load_page(base_URL + '/wiki/List_of_songs_about_Nashville,_Tennessee')\n",
    "\n",
    "start = page_soup.find(\"span\",id=\"C\").parent\n",
    "\n",
    "songs = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "for element in start.next_siblings:\n",
    "    if element.name == 'h2' and element.select('span#References'): break\n",
    "    if element.name == 'ul':\n",
    "        for li in element.select('li'):\n",
    "            song_name = get_song_name(li.getText())\n",
    "            songs.append(song_name)\n",
    "            cities.append('Nashville')\n",
    "            states.append('Tennessee')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_nashville = pd.DataFrame(data={'City':cities,'State':states,'Song':songs})\n",
    "export_data_nashville.to_csv(cwd + '/datasets/Data_nashville.csv',index=False)\n",
    "export_data_nashville.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Atlanta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = load_page(base_URL + '/wiki/List_of_songs_about_Atlanta')\n",
    "\n",
    "songs_list = page_soup.find(\"ul\")\n",
    "\n",
    "songs = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "for li in songs_list.select('li'):\n",
    "    song_name = get_song_name(li.getText())\n",
    "    songs.append(song_name)\n",
    "    cities.append('Atlanta')\n",
    "    states.append('Georgia')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_atlanta = pd.DataFrame(data={'City':cities,'State':states,'Song':songs})\n",
    "export_data_atlanta.to_csv(cwd + '/datasets/Data_atlanta.csv',index=False)\n",
    "export_data_atlanta.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Boston"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = load_page(base_URL + '/wiki/List_of_songs_about_Boston')\n",
    "\n",
    "songs_list = page_soup.find(\"ul\")\n",
    "\n",
    "songs = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "for li in songs_list.select('li'):\n",
    "    song_name = get_song_name(li.getText())\n",
    "    songs.append(song_name)\n",
    "    cities.append('Boston')\n",
    "    states.append('Massachusetts')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_boston = pd.DataFrame(data={'City':cities,'State':states,'Song':songs})\n",
    "export_data_boston.to_csv(cwd + '/datasets/Data_boston.csv',index=False)\n",
    "export_data_boston.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Portland"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "page_soup = load_page(base_URL + '/wiki/List_of_songs_about_Portland,_Oregon')\n",
    "\n",
    "songs_list = page_soup.find(\"ul\")\n",
    "\n",
    "songs = []\n",
    "cities = []\n",
    "states = []\n",
    "\n",
    "for li in songs_list.select('li'):\n",
    "    song_name = get_song_name(li.getText())\n",
    "    songs.append(song_name)\n",
    "    cities.append('Portland')\n",
    "    states.append('Oregon')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "export_data_portland = pd.DataFrame(data={'City':cities,'State':states,'Song':songs})\n",
    "export_data_portland.to_csv(cwd + '/datasets/Data_portland.csv',index=False)\n",
    "export_data_portland.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting lat and long from GeoPy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "geolocator = Nominatim(user_agent=\"my_app\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "separate_cities = [\n",
    "    {\n",
    "        'city':'Los Angeles',\n",
    "        'state':'California'\n",
    "    },\n",
    "    {\n",
    "        'city':'Atlanta',\n",
    "        'state':'Georgia'\n",
    "    },\n",
    "    {\n",
    "        'city':'Boston',\n",
    "        'state':'Massachusetts'\n",
    "    },\n",
    "    {\n",
    "        'city':'Chicago',\n",
    "        'state':'Illinois'\n",
    "    },\n",
    "    {\n",
    "        'city':'Detroit',\n",
    "        'state':'Michigan'\n",
    "    },\n",
    "    {\n",
    "        'city':'Miami',\n",
    "        'state':'Florida'\n",
    "    },\n",
    "    {\n",
    "        'city':'Nashville',\n",
    "        'state':'Tennessee'\n",
    "    },\n",
    "    {\n",
    "        'city':'New Orleans',\n",
    "        'state':'Louisiana'\n",
    "    },\n",
    "    {\n",
    "        'city':'New York City',\n",
    "        'state':'New York'\n",
    "    },\n",
    "    {\n",
    "        'city':'Portland',\n",
    "        'state':'Oregon'\n",
    "    }\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "i = 0\n",
    "while i < len(separate_cities):\n",
    "    try:\n",
    "        loc = geolocator.geocode(f'{separate_cities[i][\"city\"]},{separate_cities[i][\"state\"]} United States')\n",
    "    except Exception:\n",
    "        print(f'ðŸ›‘ Can\\'t generate coordinates for {separate_cities[i][\"city\"]}')\n",
    "        continue\n",
    "\n",
    "    dataset_to_change_PATH = cwd + f'/datasets/Data_{normalized_city_name(separate_cities[i][\"city\"])}.csv'\n",
    "\n",
    "    data_to_change = pd.read_csv(dataset_to_change_PATH, index_col=[0])\n",
    "    data_to_change['lat'] = loc.latitude\n",
    "    data_to_change['long'] = loc.longitude\n",
    "    data_to_change.to_csv(dataset_to_change_PATH)\n",
    "    print(f'âœ… Coordinates for {separate_cities[i][\"city\"]} generated!')\n",
    "    i+=1\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Get lat and long for main dataset\n",
    "\n",
    "*Note: GeoPy allows for 2500 requests per day*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_dataset_import = pd.read_csv(cwd + '/datasets/Data_main.csv')\n",
    "main_dataset = main_dataset_import.copy()\n",
    "main_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_dataset['Location Name'] = main_dataset.apply(lambda row: f'{row[\"City\"]}, {row[\"State\"]} United States', axis=1)\n",
    "main_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=2)\n",
    "\n",
    "main_dataset['Geo Location'] = main_dataset['Location Name'].apply(geocode)\n",
    "\n",
    "main_dataset['lat'] = main_dataset['Geo Location'].apply(lambda loc: loc.latitude if loc else None)\n",
    "main_dataset['long'] = main_dataset['Geo Location'].apply(lambda loc: loc.longitude if loc else None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_dataset = main_dataset.drop(columns=['Location Name','Geo Location'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_dataset.to_csv(cwd + '/datasets/Data_main.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removing unused columns for easier merge"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "main_dataset.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge all datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data = pd.concat(map(pd.read_csv, glob.glob('datasets/*.csv')), ignore_index=True)\n",
    "merged_data.to_csv(cwd + '/Data_merged.csv')\n",
    "merged_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "merged_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shutil.copy2(cwd + '/Data_merged.csv', cwd + '/../data_viz')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('webscraping': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "a0f3f1c78d9d0645b6ecda63dc91b32fd233a6ca0882f9c6740e4d53711de2e4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}